# -*- coding: utf-8 -*-
"""GOOGLE_AI_firstModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11vRX8hiPE6am7K0pXHk366t7G4Fm9ISy
"""

# Commented out IPython magic to ensure Python compatibility.
# Requirements

import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt

import numpy as np
import pandas as pd

from sklearn import metrics
# importation de la version 1 de tensor flow
# Des erreurs sont apparues avec la version 2 que je n'ai pas su corriger
# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow.python.data import Dataset

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

# parametres de print dans le terminal
pd.set_option('display.max_rows', 15)
pd.set_option('display.max_columns', 15)
pd.set_option('display.width', 1000)

pd.options.display.float_format = '{:.1f}'.format
# print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

"""On charge les données qui nous serviront de base : données sur l'immobilier en Californie

"""

california_housing_dataframe = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv", sep=",")
print(california_housing_dataframe.head(15), '\n\n')

"""Méthode de mélange aléatoire des données des données:

L'objectif est d'eviter les effets de classements pathologique pouvant nuire aux performanes de la descente de gradient stochastique
"""

california_housing_dataframe = california_housing_dataframe.reindex(np.random.permutation((california_housing_dataframe.index)))

"""
Creation de la fonction qui prepare les features à selectionner :"""

def preparation_features(dataframe):

    # Selection des features qui nous interessent
    selected_features = dataframe[[
        "latitude",
        "longitude",
        "housing_median_age",
        "total_rooms",
        "total_bedrooms",
        "population",
        "households",
        "median_income"
    ]]

    processed_features = selected_features.copy()

    processed_features["rooms_per_person"] = (dataframe["total_rooms"]/dataframe["population"])

    return processed_features

# test de la fonction 
print(preparation_features(california_housing_dataframe).head(5))

"""Creation de la fonction qui prepare les targets ( aussi appelés labels ) :

"""

def preparation_targets_labels(dataframe):

    output_targets = pd.DataFrame()
    output_targets["median_house_value"] = (dataframe["median_house_value"]/1000.0)

    return output_targets

# test de la fonction 
print(preparation_targets_labels(california_housing_dataframe).head(5))

"""
Preparation des différents ensembles de données nécessaires à l'expérience : 
"""

# Preparation des l'ensemdle d'entrainement avec les labels associés
training_exemples = preparation_features(california_housing_dataframe.head(12000))
training_targets = preparation_targets_labels(california_housing_dataframe.head(12000))

# Preparation de l'ensemble de validation ( features + labels )

validation_exemples = preparation_features(california_housing_dataframe.tail(5000))
validation_targets = preparation_targets_labels(california_housing_dataframe.tail(5000))

"""Creation de la fonction de creation de figure latitudexlongitude. Cela apporte deux informations. 

1 - Avoir une représentation géograpgique de la zone géographique couverte 

2 - Observer une repartition homogène des données entre l'ensemble d'entrainement et l'ensemble de validation 

"""

def plot_data_log_lat(vale,valt, traine,traint):

    plt.figure(figsize=(13, 8))

    ax = plt.subplot(1,2,1)
    ax.set_title("Validation Data")

    ax.set_autoscaley_on(False)
    ax.set_ylim([32,43])
    ax.set_autoscaley_on(False)
    ax.set_xlim([-126, -112])
    plt.scatter(vale["longitude"], vale["latitude"], cmap="coolwarm", c=valt["median_house_value"]/valt["median_house_value"].max())


    ax = plt.subplot(1,2,2)
    ax.set_title("training Data")
    ax.set_autoscaley_on(False)
    ax.set_ylim([32,43])
    ax.set_autoscaley_on(False)
    ax.set_xlim([-126, -112])
    plt.scatter(traine["longitude"], traine["latitude"], cmap="coolwarm",c=traint["median_house_value"] / traint["median_house_value"].max())


    plt.plot()
    # plt.show()



plot_data_log_lat(validation_exemples, validation_targets, training_exemples, training_targets)

"""
Creation de la fonction qui formate les données pour les inserer dans le modèle. 

On parle d'input_fonction. On passe nos données selectionnées en entrée et elle crée le dataset divisé en batch equivalent et renvoie les couples features, labels


Mais avant ça il faaut creer un iterateur sur les données qu'on utilisera dans cette fonction. En effet dans la v2 de tensorflow on a besoin de cette methode. Si on utilise la V1 de tensorflow alors on peut utiliser la partie commentée
"""

@tf.function
def iterateur(dataset):
  iterator = iter(dataset)
  return next(iterator)

def my_input_fonction(features, targets, batch_size=1, shuffle=True, num_epochs = None):
    # Les arguments sont :
    # features : les colonnes du dataframes qui servent de données
    # targets :  La colonne de données d'apprentissage qu'on cherche à predire plus tard
    # Si shuffle est True les données seront lues de manière aléatoire de manière à etre transmises de manière aleatoire au modèle au cours de l'apprentissage

    # Convertir les données du dataframe en un dictionnaire de tableaux
    features = {key:np.array(value) for key,value in dict(features).items()}
    # print('Dictionnaire de tableaux features : \n\n', features, '\n\n')

    #Construire un dataset et configurer le batching

    Data_set = Dataset.from_tensor_slices((features, targets))
    Data_set = Data_set.batch(batch_size).repeat(num_epochs)

    # print('This is the dataset : \n\n', Data_set, '\n\n')


    # Melanger la Data  :
    if shuffle:
        Data_set =Data_set.shuffle(10000)

    

    # Retourner la batch de données

    # features, labels = Data_set.make_one_shot_iterator().get_next() # Cette fonction n'est plus valable dans TF V2 , seulement dans V1
    features, labels = tf.compat.v1.data.make_one_shot_iterator(Data_set).get_next()
    
    # features, labels = iterateur(Data_set)

    
    # print('this is the features output : \n\n', features, '\n\n this is the labels in Output : \n\n', labels)

    return features,labels

# Test fonction : 
print(my_input_fonction(training_exemples, training_targets, batch_size=5, shuffle=True, num_epochs = 1))


# Et ça ressemble à cette données
exemple = {'feature1':['value1','value2','value3'], 'feature2':['value1','value2','value3'],'feature3':['value1','value2','value3']}
print(exemple, '\n Il y a autant de valeur par features que la taille du batch saisie')

"""Ensuite il faut créer une fonction qui va generer un ensemble de tous les types de features à utiliser. 

En effet il faut indiquer au modèle pour chaque feature qu'il croisera quel est le type des données contenu ( ex : données numériques, données textuelles , etc ... )
"""

def creation_features_description(input_features):
    return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features])

# Test fonction 
print(creation_features_description(training_exemples))

"""Creation de la fonction principale qui va entrainer le modèle 


"""

def train_model(learning_rate, steps, batch_size, training_exemples, training_targets, validation_exemples):
    # , validation_targets

    periods = 10
    steps_per_period = steps / periods

    # creation du modèle
    # my_optimizer = tf.optimizers.SGD(learning_rate=learning_rate, name='SGD', clipnorm=5.0)
    # my_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, name='SGD', clipnorm=5.0)
    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
    
    linear_regressor = tf.estimator.LinearRegressor(
      feature_columns= creation_features_description(training_exemples),
      optimizer= my_optimizer
    )   
    
    # creation des données:
    generate_final_training_data = lambda: my_input_fonction(training_exemples, training_targets["median_house_value"], batch_size=batch_size)
    
    generate_final_training_data_for_prediction = lambda: my_input_fonction(training_exemples, training_targets["median_house_value"], num_epochs=1,shuffle=False)
    generate_final_validation_data_for_prediction = lambda: my_input_fonction(validation_exemples, validation_targets["median_house_value"], num_epochs=1, shuffle=False)

    #       Entrainement du modèle :
    print("Training model...\n")
    print("RMSE (on training data):\n")

    training_rmse=[]
    validation_rmse=[]

    for period in range(0, periods):         

      #etape d'entrainement:
      linear_regressor.train(
          input_fn= generate_final_training_data,
          steps= steps_per_period,
      )

      #le reseau a été un peu entrainer on va essayer de muser les rsultat de son apprentissage:
      # On calcule alors les predictions sur le modèle entrainé :
      training_predictions = linear_regressor.predict(input_fn= generate_final_training_data_for_prediction )
      training_predictions = np.array([item['predictions'][0] for item in training_predictions])

      #on fait pareil sur les données de validation:
      validation_predictions = linear_regressor.predict(input_fn=generate_final_validation_data_for_prediction )
      validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])

      #on calcule les perte d'entrainement et la perte de validation

      training_rmse_computation = math.sqrt(metrics.mean_squared_error(training_predictions, training_targets))
      validation_rmse_computation = math.sqrt(metrics.mean_squared_error(validation_predictions, validation_targets))

      print(" period %02d : %0.2f" % (period, training_rmse_computation))
      training_rmse.append(training_rmse_computation)
      validation_rmse.append(validation_rmse_computation)

    print("Model training finished")

    plt.ylabel("RMSE")
    plt.xlabel("Periods")
    plt.title("Root Mean Squared Error vs. Periods")
    plt.tight_layout()
    plt.plot(training_rmse, label="training")
    plt.plot(validation_rmse, label="validation")
    plt.legend()
    plt.show()

    return linear_regressor

"""Lancement du modèle :"""

_ = train_model(
    learning_rate=0.00003,
    steps=500,
    batch_size=5,
    training_exemples= training_exemples,
    training_targets= training_targets,
    validation_exemples= validation_exemples    
    )
# validation_targets= validation_targets
print("DONE")

california_housing_test_data = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv", sep=",")

test_examples = preparation_features(california_housing_test_data)
test_targets = preparation_targets_labels(california_housing_test_data)

predict_test_input_fn = lambda: my_input_fonction(
      test_examples, 
      test_targets["median_house_value"], 
      num_epochs=1, 
      shuffle=False)

test_predictions = _.predict(input_fn=predict_test_input_fn)
test_predictions = np.array([item['predictions'][0] for item in test_predictions])

root_mean_squared_error = math.sqrt(
    metrics.mean_squared_error(test_predictions, test_targets))

print("Final RMSE (on test data): %0.2f" % root_mean_squared_error)